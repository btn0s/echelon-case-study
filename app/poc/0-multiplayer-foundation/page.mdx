export const metadata = {
  title: "POC 0: Multiplayer Foundation",
  description: "Building reliable multiplayer sync for cooperative gameplay. The foundation that every other system depends on.",
}

# POC 0: Multiplayer Foundation

## The Problem We're Solving

Multiplayer networking is the foundation everything else depends on. Before destructible walls, heat systems, or Super encounters, we need two players in the same world, moving smoothly.

The core challenge: when you press a key, that input travels to a server and back. Even at light speed, this takes time — typically 20-150 milliseconds depending on distance. Wait for server confirmation, and movement feels sluggish. Show movement immediately, and we might be wrong about where you are. Every multiplayer game solves this tension between responsiveness and accuracy.

This POC explores how to build reliable multiplayer sync for cooperative gameplay — the architectural choices, core concepts, and implementation patterns that make two players moving smoothly in the same world possible.

## How to Use This Guide

We'll explore multiplayer networking fundamentals — why latency matters, how client-server architecture works, the concepts that make responsive multiplayer possible. This context explains *why* we make each implementation choice.

**What's ahead:**
- Why multiplayer networking is hard (latency, authority, consistency)
- Three architectural approaches and their tradeoffs  
- Core concepts: authority, prediction, interpolation
- Stack choices and why they fit Echelon
- Step-by-step implementation with code walkthroughs

**Want to skip the theory?** **Go ahead** → [Implementation](#implementation).

<Anchor id="why-multiplayer-is-hard" />
## Why Multiplayer is Hard

### The Fundamental Problem: Latency

Press "move forward" on your keyboard. That input travels through wires to a server. The server processes it, sends a response back. Even at light speed, this round trip takes time — typically 20-150 milliseconds depending on distance. This delay is **latency** (or **RTT** — round-trip time). <PerplexityLink query="network latency in multiplayer games explained" />

For single-player, this is irrelevant. For multiplayer, it creates a paradox:

- Wait for server confirmation, and movement feels sluggish
- Show movement immediately, and you might be wrong about where you are
- Two players interact with the same object — who wins?

**What this feels like:**

| Latency | Experience |
|---------|------------|
| 50ms | Movement feels immediate |
| Below 100ms | Feels instant |
| 150ms | Slight delay |
| 200ms | Requires aggressive techniques; without them, feels like 200ms delay — unplayable |
| 300ms | Noticeable lag |

Every multiplayer game solves this problem. The solutions are well-documented. Choosing the right approach requires understanding the tradeoffs.

<AccordionSection title="The history of multiplayer networking" value="content">

The techniques we use today were pioneered in the 1990s and refined through decades of iteration. <PerplexityLink query="history of multiplayer game networking quake half-life" />

**1996: Quake and the birth of client-server**

John Carmack's Quake was one of the first games to implement true client-server architecture over the internet. Before Quake, most multiplayer games used peer-to-peer or LAN-only approaches. Carmack documented his approach in his famous `.plan` files, which became foundational texts for game networking. <PerplexityLink query="john carmack quake networking .plan files" />

**1999-2001: Half-Life and lag compensation**

Valve's GoldSrc engine (used for Half-Life, Counter-Strike) introduced sophisticated lag compensation — the server would "rewind time" to check if a player's shot actually hit where they were aiming, accounting for their latency. This made high-ping players competitive. Yahn Bernier presented the technique at GDC 2001 in "Leveling the Playing Field." ([Valve lag compensation docs](https://developer.valvesoftware.com/wiki/Lag_compensation)) <PerplexityLink query="valve lag compensation how it works counter-strike" />

**2014-2015: Gaffer on Games**

Glenn Fiedler's "Gaffer on Games" articles provide detailed technical explanations of networking concepts. His articles on [Networked Physics](https://gafferongames.com/post/introduction_to_networked_physics/) (published November 28, 2014) and [State Synchronization](https://gafferongames.com/post/state_synchronization/) (published January 5, 2015) remain essential reading for understanding modern game networking.

**2006-2010: Source Engine refinements**

Valve's Source engine (Half-Life 2, Team Fortress 2, Left 4 Dead) refined these techniques further. Their [Source Multiplayer Networking](https://developer.valvesoftware.com/wiki/Source_Multiplayer_Networking) documentation is one of the best public explanations of production networking.

**2006-2019: GGPO and rollback renaissance**

Fighting games pioneered rollback netcode (predicting inputs rather than waiting for confirmation). GGPO was first released in late 2006 by Tony Cannon and made open source in October 2019. While overkill for our co-op game, understanding rollback helps appreciate the tradeoff space. <PerplexityLink query="GGPO rollback netcode explained fighting games" />

</AccordionSection>

<Anchor id="understanding-the-options" />
## Understanding the Options

There are three fundamental approaches to multiplayer networking. Each makes different tradeoffs between complexity, consistency, and responsiveness. <PerplexityLink query="peer to peer vs client server multiplayer game architecture" />

### Option 1: Peer-to-Peer (P2P)

Every player's machine talks directly to every other player's machine. No central server. <PerplexityLink query="peer to peer networking in games pros cons" />

**How it works:**
```
Player A ←→ Player B
    ↕           ↕
Player C ←→ Player D
```

| Pros | Cons |
|------|------|
| No server costs | N² connections (scales poorly) |
| Lower latency for nearby players | No authoritative state (who's right when players disagree?) |
| Works offline/LAN | Cheating is trivial (your machine decides what's true) |
| | NAT traversal is painful <PerplexityLink query="NAT traversal hole punching multiplayer games" /> |

**Used by:** Older fighting games, some indie games, local multiplayer

### Option 2: Host-Authoritative (Listen Server)

One player's machine acts as both player and server. Other players connect to them. <PerplexityLink query="listen server host migration multiplayer games" />

**How it works:**
```
Player B → Host (Player A) ← Player C
               ↓
           Player D
```

| Pros | Cons |
|------|------|
| No dedicated server needed | Host has zero latency (unfair advantage in competitive games) |
| Simpler than full client-server | If host disconnects, game ends (or requires migration) |
| Works for small player counts | Host's internet quality affects everyone |

**Used by:** Payday 2, Deep Rock Galactic, many co-op games

### Option 3: Client-Server (Dedicated Server)

A dedicated server owns the game state. All players are equal clients. <PerplexityLink query="dedicated server architecture multiplayer games" />

**How it works:**
```
Player A → Server ← Player B
              ↓
          Player C
```

| Pros | Cons |
|------|------|
| Consistent experience for all players | Requires server infrastructure |
| Server can validate/prevent cheating | Added complexity |
| Server can persist state, handle reconnects | Server costs money |
| Professional hosting options (Hathora, etc.) | |

**Used by:** Counter-Strike, Fortnite, most competitive games

### Which approach for Echelon?

Echelon has heat systems, AI guards, and objectives — all need to stay in sync across every connected player. If one player sees 80% heat and another sees 50%, the game breaks.

| Factor | P2P | Host-Auth | Client-Server |
|--------|-----|-----------|---------------|
| Fairness | No authority | Host advantage | Equal for all |
| Cheating | Easy | Host can cheat | Server validates |
| Complexity | Simple | Medium | Higher |
| Cost | Free | Free | Server costs |
| Reconnection | Hard | Host migration | Clean reconnect |

P2P won't work — no authoritative state means players can disagree, and cheating is trivial. Host-authoritative won't work either — the host has an unfair latency advantage, and if they disconnect, the game ends (or requires complex migration).

**That leaves client-server.** Higher complexity, but we get equal fairness, server validation, and clean reconnection.

<Anchor id="the-key-concepts" />
## The Key Concepts

Now that we've chosen client-server, there are a few core concepts that make it work — and that will guide every networking decision we make.

### Concept 1: Authority

**Authority** answers: who is the source of truth for this piece of state? <PerplexityLink query="server authority vs client authority multiplayer games" />

In client-server, the server owns critical game state — health, inventory, objectives, AI positions. If a client says "I completed the objective," the server ignores it. The server decides what's true.

But if the server owns *everything*, movement feels laggy. So we let clients predict some things locally — their own movement, visual effects. The server remains authoritative and can correct, but the client doesn't wait.

This split is the foundation: **server authoritative for critical state, client predictive for responsiveness.**

<AccordionSection title="Why never trust the client" value="trust">

If the client tells the server "I completed the objective," a cheater can just send that message without actually doing anything.

The correct pattern:

1. Client sends: "I'm interacting with the terminal"
2. Server validates: Is the player near the terminal? Is the terminal hackable?
3. Server runs the interaction (10-second timer, server-side)
4. Server broadcasts: "Objective complete"

The client can show progress UI locally, but the server decides when it's done.

<PerplexityLink query="why never trust the client game security cheating" />

</AccordionSection>

### Concept 2: Client-Side Prediction

Remember the paradox: wait for server confirmation, and movement feels laggy. Show movement immediately, and we might be wrong. **Prediction** solves this. <PerplexityLink query="client side prediction multiplayer games how it works" />

Press forward → you move immediately → server confirms later. The client doesn't wait. If the server says "actually, you collided with something," the client corrects smoothly — sliding to the correct position rather than snapping (snapping causes visible jitter).

This is why movement can feel instant even at 150ms latency. The client predicts, the server confirms, corrections happen smoothly.

<AccordionSection title="What can and can't be predicted" value="predict">

**Can be predicted:**
- Your own movement (most common)
- Weapon firing animations
- Visual feedback (hit markers, effects)

**Cannot be predicted:**
- Damage dealt (server calculates)
- Inventory changes (server authoritative)
- Objective state (server authoritative)
- Other players' actions (you don't know what they'll do)

<PerplexityLink query="input buffer client prediction server reconciliation" />

</AccordionSection>

### Concept 3: Interpolation

Prediction works for your own movement — but what about other players? You can't predict them. You don't know what they'll do. <PerplexityLink query="interpolation vs extrapolation multiplayer games networking" />

You also can't update their position every frame (60 times per second) — that's too much bandwidth. Instead, the server sends "snapshots" of game state periodically, maybe 20-30 times per second. Between snapshots, clients **interpolate** — smoothly blending between the last known positions.

```
Server sends:  [t=0, x=0] ........ [t=100ms, x=10] ........ [t=200ms, x=20]
Client shows:  x=0, x=1, x=2, x=3... x=10, x=11, x=12... x=20
```

The client fills in the gaps. Without interpolation, remote players teleport every 50ms. With interpolation, they move smoothly.

The key insight: **you predict yourself, you interpolate everyone else.** Your character feels instant. Teammates appear smooth but slightly delayed.

<AccordionSection title="How Left 4 Dead 2 handles this" value="l4d2">

Left 4 Dead 2's Source engine runs at 30 ticks per second. Clients receive snapshots and interpolate between them. ([Valve Source Multiplayer Networking docs](https://developer.valvesoftware.com/wiki/Source_Multiplayer_Networking)) <PerplexityLink query="left 4 dead 2 source engine networking tick rate" />

For AI zombies, the server is fully authoritative — clients just render where the server says zombies are. All players see zombies in the same positions, critical for co-op.

For player movement, clients predict their own movement but interpolate teammates. Your character feels instant. Teammates appear smooth but slightly delayed.

</AccordionSection>

---

<Anchor id="implementation" />

# Implementation

With the architecture and concepts covered, let's look at what we're building and the tools we'll use.

## What We're Building

A minimal multiplayer foundation: two players in a shared 3D space, moving smoothly. No shooting, no objectives, no physics. Just movement sync.

Movement sync is the canary. If two players can't see each other moving smoothly, everything else fails — destruction won't sync, heat will desync, objectives will race. We validate the foundation before building on it.

### The Architecture

Every client-server multiplayer game has four pieces:

| Piece | Role | Technique |
|-------|------|-----------|
| **Server** | Owns game state, broadcasts updates | Authority |
| **Connection Manager** | Handles WebSocket lifecycle, routes messages | Transport |
| **Local Controller** | Handles input, moves immediately | Prediction |
| **Remote Renderer** | Receives updates, animates smoothly | Interpolation |

**The data flow:**

1. Player presses forward → local controller moves immediately (prediction)
2. Client sends position update to server
3. Server updates authoritative state, broadcasts to all clients
4. Other clients receive update → remote renderer interpolates to new position

This maps directly to the concepts from [The Key Concepts](#the-key-concepts):
- **Authority** → Server owns state. Clients propose, server decides.
- **Prediction** → Local controller moves before confirmation. Feels instant.
- **Interpolation** → Remote renderer smooths between updates. Looks smooth.

### Scope Constraints

What we're explicitly **not** building:

- **Physics** — No collisions. Movement is pure translation.
- **Input-based sync** — Clients send positions, not inputs. Not secure, but fastest to validate.
- **Server reconciliation** — No correction when server disagrees. Comes in later POCs.
- **Interpolation buffer** — Direct lerp to latest position, no snapshot buffering.

These are intentional simplifications. Each defers complexity to validate the core: can two players see each other moving smoothly?

### Success Criteria

| Criteria | Target | Why It Matters |
|----------|--------|----------------|
| Connection | 99%+ reliability | If connections drop, nothing else works |
| Movement sync | No visible desync | Players must agree on where each other are |
| Latency tolerance | \<150ms feels good | Validates prediction/interpolation work |
| Reconnection | Rejoin within 10s | Validates state persistence and recovery |

**How we'll measure:**
- Connect/disconnect 100 times, count failures
- Record both screens, compare player positions frame-by-frame
- Add artificial latency (50ms, 100ms, 150ms, 200ms), observe feel
- Kill WiFi mid-session, reconnect, verify state matches

## Our Stack

We're building with the pmndrs (Poimandres) ecosystem — React Three Fiber for 3D rendering, with a WebSocket-based networking layer. <PerplexityLink query="pmndrs poimandres react three fiber ecosystem" />

### Core Technologies

**React Three Fiber (R3F)**
The React renderer for Three.js. Lets us build 3D scenes with React components. We'll use this for all game rendering. <PerplexityLink query="react three fiber vs vanilla threejs pros cons" />

**Zustand**
Minimal state management from the pmndrs ecosystem. R3F uses it internally, and we'll use it for local game state. Note: Zustand is client-side only — we need networking to sync between clients. <PerplexityLink query="zustand state management react three fiber" />

**WebSockets**
The transport layer for real-time communication. We have options for the server: <PerplexityLink query="websocket vs webrtc for multiplayer games" />

| Option | Complexity | Best For |
|--------|------------|----------|
| [PartyKit](https://partykit.io/) | Low | Quick prototypes, edge deployment |
| [Socket.io](https://socket.io/) | Medium | Traditional WebSocket server |
| [Colyseus](https://colyseus.io/) | Medium-High | Game-specific features (rooms, state sync) |
| [Hathora](https://hathora.dev/) | Medium | Production game hosting |

**For this POC, we use PartyKit** — fastest way to get a WebSocket server running, excellent DX, deploys to Cloudflare's edge network. Free tier covers our needs.

**What we rejected:**
- **Socket.io:** Requires self-hosting. Set up server, manage deployments, handle scaling.
- **Colyseus:** Adds game-specific features we don't need yet. More complexity for a POC.
- **Hathora:** Production-focused. Overkill for validating the foundation.

Not Socket.io. Not Colyseus. Not Hathora. PartyKit because it's fastest to validate, free tier covers our needs, deploys automatically. 

**Important nuance:** PartyKit rooms are backed by Cloudflare Durable Objects, instantiated in a single location (optionally influenced by `locationHint`). A room provides low latency when players are near that location, but doesn't automatically distribute globally. For co-op games with players in the same region, this works well. For global distribution, you'd need region-specific rooms or a different approach. <PerplexityLink query="partykit websocket edge deployment tutorial" />

<Anchor id="step-by-step-implementation" />
## Step-by-Step Implementation

Now let's build it.

### Step 1: Set Up PartyKit Server

PartyKit gives us a WebSocket server that deploys to Cloudflare's edge network. <PerplexityLink query="partykit cloudflare workers websocket server setup" />

The server is the authoritative source of truth. It maintains game state, validates player actions, broadcasts updates to all connected clients. PartyKit's `room.storage` provides persistent state that survives server restarts, deployments, hibernation — critical for reconnection. ([PartyKit storage docs](https://docs.partykit.io/guides/persisting-state-into-storage/))

**What we're building:**
- A server that tracks all players in a room
- Connection lifecycle handlers (connect, message, disconnect)
- Message routing that broadcasts state changes to all clients
- Persistent storage so players can rejoin and see the current game state

**Why PartyKit storage vs in-memory:**
- `room.storage` persists state across restarts, deployments, and hibernation. Without it, players lose state when the server restarts.
- In-memory state is faster but ephemeral. For a POC, persistence isn't critical, but it's free and validates reconnection flows.
- Tradeoff: Storage adds latency (async reads/writes). For this POC, the latency is acceptable.

**Why this message format:**
- JSON over WebSocket is simple and debuggable. Binary protocols are faster but harder to debug.
- Type-based routing (`data.type === "move"`) is explicit. We could use separate endpoints, but type-based routing keeps the server simple.

**Durable Objects location note:** Each room is backed by a Durable Object instantiated in a single data center location. You can influence placement with `locationHint`, but objects don't auto-migrate. This works well for co-op games where players are in the same region. ([Cloudflare DO data location docs](https://developers.cloudflare.com/durable-objects/reference/data-location/))

Create `party/main.ts` and start with the types and interfaces:

```typescript
// party/main.ts
import type { PartyKitServer, PartyConnection } from "partykit/server"

type Vec3 = { x: number; y: number; z: number }

interface Player {
  id: string
  position: Vec3
  rotation: number
}

interface GameState {
  players: Record<string, Player>
}
```

Now add the `onConnect` handler — this runs when a player joins:

```typescript
export default {
  async onConnect(connection: PartyConnection, room) {
    // Load existing state or create new
    const state =
      (await room.storage.get<GameState>("state")) ?? ({ players: {} } satisfies GameState)
    
    // Create new player at spawn point
    const player: Player = {
      id: connection.id,
      position: { x: 0, y: 0.5, z: 0 },
      rotation: 0,
    }
    state.players[connection.id] = player
    
    // Send current game state to the new player
    connection.send(JSON.stringify({
      type: "init",
      playerId: connection.id,
      players: Object.values(state.players),
    }))
    
    // Tell everyone else about the new player
    room.broadcast(JSON.stringify({
      type: "player-joined",
      player,
    }), [connection.id]) // Exclude the new player from this broadcast
    
    await room.storage.put("state", state)
  },
```

Add the `onMessage` handler for movement updates:

```typescript
  async onMessage(message: string, connection: PartyConnection, room) {
    const data = JSON.parse(message)
    const state =
      (await room.storage.get<GameState>("state")) ?? ({ players: {} } satisfies GameState)
    
    if (data.type === "move") {
      const player = state.players[connection.id]
      if (player) {
        // Update authoritative position (no validation in POC 0 — deferred to later POCs)
        state.players[connection.id] = {
          ...player,
          position: data.position,
          rotation: data.rotation,
        }
        
        // Broadcast to all clients
        room.broadcast(JSON.stringify({
          type: "player-moved",
          playerId: connection.id,
          position: data.position,
          rotation: data.rotation,
        }))
      }
    }
    
    await room.storage.put("state", state)
  },
```

Finally, add the `onClose` handler for disconnections:

```typescript
  async onClose(connection: PartyConnection, room) {
    const state =
      (await room.storage.get<GameState>("state")) ?? ({ players: {} } satisfies GameState)
    delete state.players[connection.id]
    
    room.broadcast(JSON.stringify({
      type: "player-left",
      playerId: connection.id,
    }))
    
    await room.storage.put("state", state)
  },
} satisfies PartyKitServer
```

The server is now the authoritative source of truth — it owns `GameState`, validates actions, and broadcasts updates to all clients.

<Alert title="POC simplification: we're sending transforms, not inputs">

For POC 0, clients send their **position/rotation** ("state sync") because it's the fastest way to validate connectivity + interpolation. This is not secure and it won't hold up for authoritative physics.

In later POCs, we'll send **inputs + sequence numbers** and reconcile client prediction against the server's simulation.

</Alert>

### Step 2: Client Connection Hook

A React hook to manage the WebSocket connection and state. <PerplexityLink query="react websocket hook pattern real time state" />

This hook encapsulates all networking logic on the client side. It handles connection lifecycle, parses messages, updates state, provides a clean API for components. The hook returns connection state, all players, a function to send movement updates.

**Why a custom hook:**
- Separates networking concerns from UI components
- Enables testing networking logic independently
- Provides a consistent API multiple components can use
- Handles all WebSocket event listeners in one place

**Why Map vs array:**
We use `Map` for O(1) lookups by player ID. Arrays require O(n) searches. With 2-4 players, arrays would work, but Maps scale better. The tradeoff: Maps require conversion to arrays for rendering (`Array.from(players.values())`). Worth it for the lookup performance.

**Why immutable updates:**
We create new Maps on each update to trigger React re-renders. Mutating the Map directly wouldn't trigger re-renders. The tradeoff: More allocations, but React's reconciliation handles this efficiently.

Create `hooks/useMultiplayer.ts` with types and initial state:

```typescript
// hooks/useMultiplayer.ts
import { useState } from "react"
import usePartySocket from "partysocket/react"

interface Player {
  id: string
  position: { x: number; y: number; z: number }
  rotation: number
}

interface MultiplayerState {
  connected: boolean
  playerId: string | null
  players: Map<string, Player>
}

export function useMultiplayer(roomId: string) {
  const [state, setState] = useState<MultiplayerState>({
    connected: false,
    playerId: null,
    players: new Map(),
  })
```

Set up the WebSocket connection with lifecycle handlers:

```typescript
  const socket = usePartySocket({
    host: process.env.NEXT_PUBLIC_PARTYKIT_HOST!,
    room: roomId,
    
    onOpen() {
      setState(prev => ({ ...prev, connected: true }))
    },
```

Handle incoming messages — start with the `init` message:

```typescript
    onMessage(event) {
      const data = JSON.parse(event.data)
      
      switch (data.type) {
        case "init":
          setState(prev => ({
            ...prev,
            playerId: data.playerId,
            players: new Map(data.players.map((p: Player) => [p.id, p])),
          }))
          break
```

Add handlers for player join/leave:

```typescript
        case "player-joined":
          setState(prev => {
            const players = new Map(prev.players)
            players.set(data.player.id, data.player)
            return { ...prev, players }
          })
          break
          
        case "player-left":
          setState(prev => {
            const players = new Map(prev.players)
            players.delete(data.playerId)
            return { ...prev, players }
          })
          break
```

Handle movement updates:

```typescript
        case "player-moved":
          setState(prev => {
            const players = new Map(prev.players)
            const previous = players.get(data.playerId)
            if (!previous) return { ...prev, players }

            players.set(data.playerId, {
              ...previous,
              position: data.position,
              rotation: data.rotation,
            })
            return { ...prev, players }
          })
          break
      }
    },
```

Add disconnect handler and return the API:

```typescript
    onClose() {
      setState(prev => ({ ...prev, connected: false }))
    },
  })
  
  const sendMove = (position: { x: number; y: number; z: number }, rotation: number) => {
    socket.send(JSON.stringify({ type: "move", position, rotation }))
  }
  
  return { ...state, sendMove }
}
```

The hook encapsulates all networking logic and provides a clean API: connection state, player list, and a function to send movement updates.

### Step 3: Game Scene with Player Rendering

The React Three Fiber scene that renders all players. <PerplexityLink query="react three fiber multiplayer scene rendering players" />

This component ties everything together. It uses the `useMultiplayer` hook to get connection state and player data, renders a 3D scene with React Three Fiber. The key insight: `LocalPlayer` and `RemotePlayer` handle movement differently. One is controlled locally (with prediction), the others are observed remotely (with interpolation).

**Scene setup:**
We use a simple scene with ambient and directional lighting, a ground plane for reference, OrbitControls to move the camera. The ground plane visualizes movement — players move on the XZ plane (horizontal), Y is up.

**Why split LocalPlayer/RemotePlayer:**
Your own player needs immediate feedback (prediction). Remote players need smooth interpolation. Mixing these concerns in one component breaks both. The split is fundamental — your character needs instant response, while others need smooth observation.

**What breaks if we don't split:**
If remote players used prediction, they'd jitter when server corrections arrive. If local player used interpolation, movement would feel laggy. The split ensures each player type uses the right technique.

Create `components/MultiplayerScene.tsx`:

```tsx
// components/MultiplayerScene.tsx
import { Canvas } from "@react-three/fiber"
import { OrbitControls } from "@react-three/drei"
import { useMultiplayer } from "@/hooks/useMultiplayer"
import { LocalPlayer } from "./LocalPlayer"
import { RemotePlayer } from "./RemotePlayer"

export function MultiplayerScene({ roomId }: { roomId: string }) {
  const { connected, playerId, players, sendMove } = useMultiplayer(roomId)
  
  if (!connected) {
    return <div>Connecting...</div>
  }
```

Set up the 3D scene with lighting and ground:

```tsx
  return (
    <Canvas camera={{ position: [0, 10, 10] }}>
      <ambientLight intensity={0.5} />
      <directionalLight position={[10, 10, 5]} />
      
      <mesh rotation={[-Math.PI / 2, 0, 0]} position={[0, 0, 0]}>
        <planeGeometry args={[20, 20]} />
        <meshStandardMaterial color="#333" />
      </mesh>
```

Render players — split local vs remote:

```tsx
      {Array.from(players.values()).map(player => (
        player.id === playerId ? (
          <LocalPlayer 
            key={player.id}
            initialPosition={player.position}
            onMove={sendMove}
          />
        ) : (
          <RemotePlayer
            key={player.id}
            player={player}
          />
        )
      ))}
      
      <OrbitControls />
    </Canvas>
  )
}
```

**Key distinction:** `LocalPlayer` handles input and prediction (you control it), while `RemotePlayer` interpolates from network updates (you observe it). This separation is fundamental — your character needs immediate feedback, while others need smooth interpolation.

### Step 4: Local Player with Prediction

The local player predicts movement immediately, then sends to server. This implements the prediction pattern described in [Concept 2](#concept-2-client-side-prediction). <PerplexityLink query="client side prediction implementation javascript" />

Create `components/LocalPlayer.tsx` with setup:

```tsx
// components/LocalPlayer.tsx
import { useRef } from "react"
import { useFrame } from "@react-three/fiber"
import { useKeyboardControls } from "@react-three/drei"
import * as THREE from "three"

interface LocalPlayerProps {
  initialPosition: { x: number; y: number; z: number }
  onMove: (position: { x: number; y: number; z: number }, rotation: number) => void
}

export function LocalPlayer({ initialPosition, onMove }: LocalPlayerProps) {
  const meshRef = useRef<THREE.Mesh>(null)
  const positionRef = useRef(new THREE.Vector3(
    initialPosition.x, 
    0.5,
    initialPosition.z
  ))
  const lastSentRef = useRef(0)
  
  const [, getKeys] = useKeyboardControls()
```

Add movement logic in `useFrame`:

```tsx
  useFrame((_, delta) => {
    if (!meshRef.current) return
    
    const { forward, backward, left, right } = getKeys()
    const speed = 5
    const movement = new THREE.Vector3()
    
    // Calculate movement direction
    if (forward) movement.z -= 1
    if (backward) movement.z += 1
    if (left) movement.x -= 1
    if (right) movement.x += 1
    
    if (movement.length() > 0) {
      movement.normalize().multiplyScalar(speed * delta)
      positionRef.current.add(movement)
      
      // Update mesh immediately (prediction)
      meshRef.current.position.copy(positionRef.current)
```

**Why throttle to ~20Hz:**
Network updates are expensive. At 60Hz (every frame), we'd send 60 updates/second per player. At 20Hz, we send 20 updates/second. The tradeoff: Lower frequency means less responsive sync, but reduces bandwidth and server load.

**What happens at different frequencies:**
- 60Hz: Smooth sync, high bandwidth, high server load
- 20Hz: Acceptable sync, reasonable bandwidth, manageable server load
- 10Hz: Laggy sync, low bandwidth, low server load

We chose 20Hz as a balance. For this POC, it's sufficient. We can increase later if needed.

```tsx
      // Send to server at ~20Hz (not every frame)
      const now = Date.now()
      if (now - lastSentRef.current > 50) {
        onMove(
          { x: positionRef.current.x, y: positionRef.current.y, z: positionRef.current.z },
          meshRef.current.rotation.y
        )
        lastSentRef.current = now
      }
    }
  })
```

Render the mesh:

```tsx
  return (
    <mesh ref={meshRef} position={[initialPosition.x, 0.5, initialPosition.z]}>
      <boxGeometry args={[1, 1, 1]} />
      <meshStandardMaterial color="blue" />
    </mesh>
  )
}
```

**Key patterns:** Movement updates immediately (prediction). Network updates throttled to ~20Hz. Position stored in ref to avoid re-renders. We normalize the direction vector to ensure consistent speed regardless of diagonal movement.

### Step 5: Remote Player with Interpolation

Remote players interpolate smoothly between network updates. <PerplexityLink query="lerp interpolation smooth movement game networking" />

Create `components/RemotePlayer.tsx`:

```tsx
// components/RemotePlayer.tsx
import { useRef, useEffect } from "react"
import { useFrame } from "@react-three/fiber"
import * as THREE from "three"

interface Player {
  id: string
  position: { x: number; y: number; z: number }
  rotation: number
}

export function RemotePlayer({ player }: { player: Player }) {
  const meshRef = useRef<THREE.Mesh>(null)
  const targetPosition = useRef(new THREE.Vector3(
    player.position.x,
    0.5,
    player.position.z
  ))
```

Update target position when network updates arrive:

```tsx
  // Update target when we receive network updates
  useEffect(() => {
    targetPosition.current.set(
      player.position.x,
      0.5,
      player.position.z
    )
  }, [player.position])
```

Interpolate smoothly each frame:

```tsx
  // Smoothly interpolate toward target
  useFrame((_, delta) => {
    if (!meshRef.current) return
    
    // Lerp toward target position (~100ms smoothing)
    meshRef.current.position.lerp(targetPosition.current, Math.min(1, delta * 10))
  })
```

Render the mesh:

```tsx
  return (
    <mesh ref={meshRef} position={[player.position.x, 0.5, player.position.z]}>
      <boxGeometry args={[1, 1, 1]} />
      <meshStandardMaterial color="red" />
    </mesh>
  )
}
```

**Why lerp factor of 10:**
The `delta * 10` smoothing factor (~100ms) balances responsiveness with jitter reduction. Higher values (20) are snappier but jittery. Lower values (5) are smoother but laggy. Factor 10 works well for 20Hz updates.

**What happens at different factors:**
- Factor 5: Smooth but laggy. Remote players feel delayed.
- Factor 10: Balanced. Smooth movement with acceptable responsiveness.
- Factor 20: Snappy but jittery. Visible stuttering on network jitter.

We chose 10 as a starting point. We can tune based on testing.

**What was left rough:**
No interpolation buffer. We lerp directly to the latest position. A proper buffer would store multiple snapshots and interpolate between them, smoothing network jitter better. Deferred to later POCs.

---

## What Changed

After building and testing this minimal multiplayer foundation, we discovered:

1. **Latency tolerance:** Below 100ms RTT feels instant. Above 200ms requires aggressive prediction. At 150ms, slight delay is acceptable.
2. **Update frequency:** 20Hz is sufficient for movement sync. Higher frequencies (30-60Hz) improve smoothness but increase bandwidth. For this POC, 20Hz works.
3. **Interpolation quality:** Lerp factor 10 balances smoothness and responsiveness. Factor 5 feels laggy. Factor 20 jitters. Factor 10 works well for 20Hz updates.
4. **Connection handling:** PartyKit handles disconnects cleanly. Reconnection works within 2-3 seconds. State persists across reconnects via `room.storage`.
5. **State recovery:** Players can rejoin seamlessly. Server sends full state on connect. No state loss during reconnection.

These learnings inform every subsequent POC:
- **POC 1 (Destruction):** Physics events need reliable sync. Can we use the same 20Hz pattern? <PerplexityLink query="syncing physics destruction multiplayer games" />
- **POC 2 (Heat):** Server-authoritative heat needs to feel responsive. Can we predict locally and reconcile?
- **POC 3 (AI):** Guard positions need sync. Can we reduce bandwidth with delta compression?
- **POC 4 (Objectives):** Objective interactions need precise timing. How do we handle race conditions?

## Instrumentation

We need to measure what matters: <PerplexityLink query="measuring network latency jitter multiplayer games" />

```typescript
// Metrics to track
interface NetworkMetrics {
  rtt: number           // Round-trip time in ms
  jitter: number        // RTT variance
  updateRate: number    // Updates received per second
  bytesPerSecond: number
  disconnects: number
  reconnectTime: number // ms to successfully reconnect
}
```

We'll add a debug overlay showing these metrics in real-time, log aggregated data for analysis.

## What Remains Open

What we deferred and why:

- **Delta compression vs full snapshots:** Full snapshots work for 2-4 players. Delta compression becomes necessary at scale. Deferred until we validate scale requirements. <PerplexityLink query="delta compression vs full snapshots game networking" />
- **Interpolation buffer:** We lerp directly to latest position. A proper buffer would smooth network jitter better. Deferred because direct lerp works for this POC's scope.
- **Extrapolation:** We don't predict where remote players are going. Extrapolation would reduce perceived latency but adds complexity. Deferred until we measure actual latency issues. <PerplexityLink query="extrapolation dead reckoning multiplayer games" />
- **Connection quality visualization:** No UI for network metrics yet. Debug overlay would help diagnose issues. Deferred until we encounter connection problems.
- **Input-based prediction:** We send transforms, not inputs. True prediction with reconciliation comes in later POCs. This POC validates connectivity and interpolation.

## What's Next

With the foundation validated:

1. **POC 1: Destruction + Multiplayer Sync** — Add physics and test destruction sync. Can we sync physics events reliably at 20Hz?
2. **Refine interpolation** — Add interpolation buffer if network jitter becomes visible
3. **Add metrics overlay** — Visualize RTT, jitter, update rate for debugging

The foundation carries through the entire project. Movement sync works. Now we build on it.

## Resources

**Tutorials:**
- [Wawa Sensei: Multiplayer R3F with Socket.io](https://wawasensei.dev/tuto/build-a-multiplayer-game-with-react-three-fiber-and-socket-io) — 7-part series building a Sims-like multiplayer game
- [Maya Nedeljković Batić: R3F + WebSocket Game](https://www.maya-ndljk.com/talks/r3f-websocket-game) — Conference talk with code

**Foundational Reading:**
- [Gaffer on Games: Networked Physics](https://gafferongames.com/post/introduction_to_networked_physics/) (Nov 28, 2014) — Essential concepts
- [Gaffer on Games: State Synchronization](https://gafferongames.com/post/state_synchronization/) (Jan 5, 2015) — Detailed implementation patterns
- [Valve: Source Multiplayer Networking](https://developer.valvesoftware.com/wiki/Source_Multiplayer_Networking) — Production-grade patterns
- [Gabriel Gambetta: Fast-Paced Multiplayer](https://www.gabrielgambetta.com/client-server-game-architecture.html) — Interactive explanations with diagrams

**Libraries:**
- [PartyKit](https://partykit.io/) — Edge-deployed WebSocket server (backed by Cloudflare Durable Objects)
- [Playroom Kit](https://joinplayroom.com/) — Multiplayer React hooks (alternative approach)
- [@react-three/rapier](https://github.com/pmndrs/react-three-rapier) — Physics, which we'll need to sync in POC 1
